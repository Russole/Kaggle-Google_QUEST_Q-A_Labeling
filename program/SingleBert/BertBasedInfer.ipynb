{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "#from transformers import *\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from math import floor, ceil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/google-quest-challenge/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../input/google-quest-challenge/test.csv').fillna(' ')\n",
    "sub = pd.read_csv('../input/google-quest-challenge/sample_submission.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n",
       "       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n",
       "       'url', 'category', 'host', 'question_asker_intent_understanding',\n",
       "       'question_body_critical', 'question_conversational',\n",
       "       'question_expect_short_answer', 'question_fact_seeking',\n",
       "       'question_has_commonly_accepted_answer',\n",
       "       'question_interestingness_others', 'question_interestingness_self',\n",
       "       'question_multi_intent', 'question_not_really_a_question',\n",
       "       'question_opinion_seeking', 'question_type_choice',\n",
       "       'question_type_compare', 'question_type_consequence',\n",
       "       'question_type_definition', 'question_type_entity',\n",
       "       'question_type_instructions', 'question_type_procedure',\n",
       "       'question_type_reason_explanation', 'question_type_spelling',\n",
       "       'question_well_written', 'answer_helpful',\n",
       "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
       "       'answer_satisfaction', 'answer_type_instructions',\n",
       "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
       "       'answer_well_written'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_segments_xlnet(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"<sep>\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return [0] * (max_seq_length - len(tokens)) + segments\n",
    "\n",
    "def _get_segments_bert(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))#padding\n",
    "\n",
    "def _get_ids_xlnet(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids =  [5] * (max_seq_length-len(token_ids)) + token_ids\n",
    "    return input_ids\n",
    "\n",
    "def _get_ids_bert(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))#padding\n",
    "    return input_ids\n",
    "\n",
    "def _trim_input(title, question, answer, max_sequence_length=512-1, \n",
    "                t_max_len=70-1, q_max_len=219, a_max_len=219):\n",
    "\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "    # print(type(t))\n",
    "    # print(t)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:#???\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q = norm_token_length(q, q_new_len)#??\n",
    "        a = norm_token_length(a, a_new_len)#??\n",
    "        \n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "def norm_token_length(tokens, l):\n",
    "    if len(tokens) > l:\n",
    "        head = l//2\n",
    "        tail = l - head\n",
    "        return tokens[:head] + tokens[-tail:]\n",
    "    else:\n",
    "        return tokens[:l]\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, cate, pretrained_weights, max_sequence_length=512):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if \"bert-base\" in pretrained_weights:\n",
    "        stoken = [\"[CLS]\"] + [cate] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]#type:list\n",
    "        # print(stoken)\n",
    "        # print(\"==================\")\n",
    "        input_ids = _get_ids_bert(stoken, tokenizer, max_sequence_length)\n",
    "        input_segments = _get_segments_bert(stoken, max_sequence_length)\n",
    "        # print(input_ids)\n",
    "        # print(\"==================\")\n",
    "        # print(input_segments)\n",
    "        # print(\"==================\")\n",
    "\n",
    "    elif pretrained_weights == \"xlnet-base-cased\":\n",
    "        stoken = [cate] + title + [\"<sep>\"] + question + [\"<sep>\"] + answer + [\"<sep>\", \"<cls>\"]\n",
    "        input_ids = _get_ids_xlnet(stoken, tokenizer, max_sequence_length)\n",
    "        input_segments = _get_segments_xlnet(stoken, max_sequence_length)\n",
    "        try:\n",
    "            cls_index = input_segments.index(5) - 1\n",
    "        except ValueError:\n",
    "            cls_index = -1\n",
    "        input_segments[cls_index] = 2\n",
    "    \n",
    "    return [input_ids, input_segments]\n",
    "\n",
    "def convert_row(row, pretrained_weights):\n",
    "    # print(row)\n",
    "    # print(\"==================\")\n",
    "    if pretrained_weights == \"bert-base-uncased\":\n",
    "        c = f\"[{row['category'].lower()}]\"#type:str\n",
    "    elif pretrained_weights == \"bert-base-cased\":\n",
    "        c = f\"[{row['category']}]\"#type:str\n",
    "    elif pretrained_weights == \"xlnet-base-cased\":\n",
    "        c = f\"[{row['category']}]\"#type:str\n",
    "    t, q, a = title = row[\"question_title\"], row[\"question_body\"], row[\"answer\"]#type:str\n",
    "    # print(type(t))\n",
    "    # print(t)\n",
    "    # print(c)\n",
    "    # print(type(c))\n",
    "    #print(\"=====================\")\n",
    "    t, q, a = _trim_input(t, q, a)\n",
    "    ids, segments = _convert_to_bert_inputs(t, q, a, c, pretrained_weights)\n",
    "    \n",
    "    return np.array([[ids, segments]])#shape:(1, 2, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=\"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./token_model_config/tokenizer/\")\n",
    "#tokenizer.add_tokens(\"./bert_based_tokenizer/added_tokens.json\")\n",
    "#tokenizer.add_tokens(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data, targets):\n",
    "    mse = nn.MSELoss(reduction=\"none\")(data[:,:30].sigmoid(), targets[:,:30])\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='none')(data[:,:30], targets[:,:30])\n",
    "    w =  targets[:,30:]\n",
    "    loss = (mse*w).sum() + bce.sum()\n",
    "    return loss\n",
    "\n",
    "class CustomBert(nn.Module):\n",
    "    def __init__(self, config_path=None):\n",
    "        super(CustomBert, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(config_path) \n",
    "        #self.config = torch.load(config_path,output_hidden_states=True)\n",
    "        self.config.num_labels = 30\n",
    "        self.config.output_hidden_states = True\n",
    "        self.n_use_layer = 4\n",
    "        self.n_labels = self.config.num_labels\n",
    "        #self.config.save_pretrained(\"bert_based_config\")\n",
    "        #self.bert = BertModel(config)\n",
    "        self.bert=AutoModel.from_config(self.config)\n",
    "        #self.bert.save_pretrained('bert_based_model')\n",
    "        self.dense1 = nn.Linear(self.config.hidden_size*self.n_use_layer, self.config.hidden_size*self.n_use_layer)\n",
    "        self.dense2 = nn.Linear(self.config.hidden_size*self.n_use_layer, self.config.hidden_size*self.n_use_layer)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size*self.n_use_layer, self.config.num_labels)\n",
    "        #self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "\n",
    "        # outputs = self.bert(input_ids,\n",
    "        #                     attention_mask=attention_mask,\n",
    "        #                     token_type_ids=token_type_ids,\n",
    "        #                     position_ids=position_ids,\n",
    "        #                     head_mask=head_mask,\n",
    "        #                     inputs_embeds=inputs_embeds)\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                            )\n",
    "        \n",
    "        pooled_output = torch.cat([outputs[2][-1*(i+1)][:,0] for i in range(self.n_use_layer)], dim=1)\n",
    "        pooled_output = self.dense1(pooled_output)\n",
    "        pooled_output = self.dense2(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CustomBert(\"token_model_config/config/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4324 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = 'bert-base-cased'\n",
    "#X_test = test[[\"question_title\", \"question_body\", \"answer\", \"category\"]].progress_apply(lambda x: convert_row(x, pretrained_weights), axis=1).values#shape(476)\n",
    "X_test = test[[\"question_title\", \"question_body\", \"answer\", \"category\"]].apply(lambda x: convert_row(x, pretrained_weights), axis=1).values#shape(476)\n",
    "#np.vstack(X_test).shape : (476, 2, 512)\n",
    "X_test = np.vstack(X_test).reshape((len(X_test), 1024))\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29001, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dense1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "  (dense2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=3072, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.bert.resize_token_embeddings(len(tokenizer))#??\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir_target = \"../input/Optimize binning/bert-base-cased\"#\n",
    "\n",
    "cased_pred_lst = []\n",
    "for fold in range(10):\n",
    "    # if fold in [0,1,2,3,4,5,6,7]:\n",
    "    #     continue\n",
    "    #bert_path = f\"{model_dir_target}/bert-base-cased_f{fold}_best\"\n",
    "    #bert_path=f\"./bert_based_case/no_bce_no_opt_binning/bert-based-case_f{fold}_best\"\n",
    "    #solution_26th_path=f\"C:/Users/Lab000/Desktop/kaggle/kaggle_competetion/Google_Quest_LABEL/26th-solution/inference/input/quest-bertcased-10fold/bert-base-cased_f{fold}_best\"\n",
    "    solution_26th_path=f\"C:/Users/Lab000/Desktop/kaggle/kaggle_competetion/Google_Quest_LABEL/26th-solution/KaggleQuest-master/work/NoInit/BertBaseCased/bert-base-cased_f{fold}_best\"\n",
    "    model.load_state_dict(torch.load(solution_26th_path),strict=False)\n",
    "    \n",
    "    \n",
    "    lst = []\n",
    "    for i, (x_batch,)  in enumerate(test_loader):\n",
    "        input_ids = x_batch[:, :512]\n",
    "        token_ids = x_batch[:, 512:]\n",
    "        pred = model(input_ids.to(device), attention_mask=(input_ids > 0).to(device), token_type_ids=token_ids.to(device))\n",
    "        \n",
    "        lst.append(pred[0].detach().cpu().squeeze().numpy())\n",
    "    test_pred = np.vstack(lst)#shape:(476, 30)\n",
    "    \n",
    "    cased_pred_lst.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cased_pred_lst[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.9296746 ,  0.69095033, -1.6985661 , ..., -3.3332076 ,\n",
       "         2.0756507 ,  2.4260817 ],\n",
       "       [ 1.6843027 ,  0.07697463, -6.058629  , ..., -1.9949615 ,\n",
       "        -2.7619004 ,  1.7309611 ],\n",
       "       [ 2.5567524 ,  1.1753161 , -4.4454217 , ..., -2.906634  ,\n",
       "         2.1950786 ,  2.5745742 ],\n",
       "       ...,\n",
       "       [ 1.7081602 , -0.39062575, -3.994512  , ..., -1.6929474 ,\n",
       "        -0.02041219,  2.5661967 ],\n",
       "       [ 2.6966932 ,  1.3558286 , -3.8374982 , ..., -2.8559844 ,\n",
       "         1.9818013 ,  2.89408   ],\n",
       "       [ 2.4720356 ,  0.6669997 , -5.045515  , ..., -2.0027814 ,\n",
       "        -1.6465935 ,  2.1118333 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cased_pred_lst).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.949294  , 0.66617835, 0.15465264, ..., 0.03444938, 0.888514  ,\n",
       "        0.91879463],\n",
       "       [0.84347343, 0.5192342 , 0.00233215, ..., 0.11973295, 0.05941806,\n",
       "        0.8495353 ],\n",
       "       [0.9280258 , 0.76410455, 0.01159611, ..., 0.05182659, 0.8998067 ,\n",
       "        0.92920715],\n",
       "       ...,\n",
       "       [0.8465975 , 0.4035667 , 0.0180834 , ..., 0.15538862, 0.49489716,\n",
       "        0.92865413],\n",
       "       [0.93683124, 0.79508096, 0.02109294, ..., 0.05437279, 0.87887305,\n",
       "        0.947553  ],\n",
       "       [0.92215806, 0.66083103, 0.00639696, ..., 0.11891119, 0.16156988,\n",
       "        0.892048  ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "cased_pred = np.array(cased_pred_lst).mean(0)\n",
    "cased_pred = sigmoid(cased_pred)\n",
    "cased_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.949294</td>\n",
       "      <td>0.666178</td>\n",
       "      <td>0.154653</td>\n",
       "      <td>0.553623</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.630073</td>\n",
       "      <td>0.679814</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.546083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913388</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.517548</td>\n",
       "      <td>0.960436</td>\n",
       "      <td>0.964714</td>\n",
       "      <td>0.811419</td>\n",
       "      <td>0.042888</td>\n",
       "      <td>0.034449</td>\n",
       "      <td>0.888514</td>\n",
       "      <td>0.918795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.843473</td>\n",
       "      <td>0.519234</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.760515</td>\n",
       "      <td>0.813664</td>\n",
       "      <td>0.948303</td>\n",
       "      <td>0.564785</td>\n",
       "      <td>0.500868</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652297</td>\n",
       "      <td>0.947492</td>\n",
       "      <td>0.625975</td>\n",
       "      <td>0.970675</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.887246</td>\n",
       "      <td>0.933648</td>\n",
       "      <td>0.119733</td>\n",
       "      <td>0.059418</td>\n",
       "      <td>0.849535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.928026</td>\n",
       "      <td>0.764105</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.825836</td>\n",
       "      <td>0.892597</td>\n",
       "      <td>0.962454</td>\n",
       "      <td>0.628095</td>\n",
       "      <td>0.482132</td>\n",
       "      <td>0.100511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906555</td>\n",
       "      <td>0.910600</td>\n",
       "      <td>0.598329</td>\n",
       "      <td>0.961390</td>\n",
       "      <td>0.946694</td>\n",
       "      <td>0.835070</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>0.051827</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.929207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.916074</td>\n",
       "      <td>0.458491</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.729436</td>\n",
       "      <td>0.770467</td>\n",
       "      <td>0.930743</td>\n",
       "      <td>0.594157</td>\n",
       "      <td>0.463071</td>\n",
       "      <td>0.130447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726541</td>\n",
       "      <td>0.958902</td>\n",
       "      <td>0.683613</td>\n",
       "      <td>0.975063</td>\n",
       "      <td>0.986992</td>\n",
       "      <td>0.908914</td>\n",
       "      <td>0.881619</td>\n",
       "      <td>0.154653</td>\n",
       "      <td>0.627130</td>\n",
       "      <td>0.902189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.939911</td>\n",
       "      <td>0.481985</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>0.819952</td>\n",
       "      <td>0.672361</td>\n",
       "      <td>0.842770</td>\n",
       "      <td>0.628446</td>\n",
       "      <td>0.630961</td>\n",
       "      <td>0.253318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695076</td>\n",
       "      <td>0.918483</td>\n",
       "      <td>0.667006</td>\n",
       "      <td>0.963846</td>\n",
       "      <td>0.962784</td>\n",
       "      <td>0.830125</td>\n",
       "      <td>0.230669</td>\n",
       "      <td>0.139842</td>\n",
       "      <td>0.706315</td>\n",
       "      <td>0.882781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.949294                0.666178   \n",
       "1     46                             0.843473                0.519234   \n",
       "2     70                             0.928026                0.764105   \n",
       "3    132                             0.916074                0.458491   \n",
       "4    200                             0.939911                0.481985   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.154653                      0.553623   \n",
       "1                 0.002332                      0.760515   \n",
       "2                 0.011596                      0.825836   \n",
       "3                 0.002027                      0.729436   \n",
       "4                 0.040984                      0.819952   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.604500                               0.630073   \n",
       "1               0.813664                               0.948303   \n",
       "2               0.892597                               0.962454   \n",
       "3               0.770467                               0.930743   \n",
       "4               0.672361                               0.842770   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.679814                       0.669111   \n",
       "1                         0.564785                       0.500868   \n",
       "2                         0.628095                       0.482132   \n",
       "3                         0.594157                       0.463071   \n",
       "4                         0.628446                       0.630961   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.546083  ...               0.913388        0.900747   \n",
       "1               0.103378  ...               0.652297        0.947492   \n",
       "2               0.100511  ...               0.906555        0.910600   \n",
       "3               0.130447  ...               0.726541        0.958902   \n",
       "4               0.253318  ...               0.695076        0.918483   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.517548          0.960436          0.964714   \n",
       "1                     0.625975          0.970675          0.981972   \n",
       "2                     0.598329          0.961390          0.946694   \n",
       "3                     0.683613          0.975063          0.986992   \n",
       "4                     0.667006          0.963846          0.962784   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.811419                  0.042888               0.034449   \n",
       "1             0.887246                  0.933648               0.119733   \n",
       "2             0.835070                  0.065642               0.051827   \n",
       "3             0.908914                  0.881619               0.154653   \n",
       "4             0.830125                  0.230669               0.139842   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.888514             0.918795  \n",
       "1                        0.059418             0.849535  \n",
       "2                        0.899807             0.929207  \n",
       "3                        0.627130             0.902189  \n",
       "4                        0.706315             0.882781  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.columns[1:]] = cased_pred\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "490296f01f129d1791d2b0949f01d8bcbb5336528260279af7e468eefab7c316"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
