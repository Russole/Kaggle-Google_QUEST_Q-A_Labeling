{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "#from transformers import *\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from math import floor, ceil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Lab000/Desktop/kaggle/kaggle_competetion/Google_Quest_LABEL/my-solution/input/google-quest-challenge/train.csv').fillna(' ')\n",
    "test = pd.read_csv('C:/Users/Lab000/Desktop/kaggle/kaggle_competetion/Google_Quest_LABEL/my-solution/input/google-quest-challenge/test.csv').fillna(' ')\n",
    "sub = pd.read_csv('C:/Users/Lab000/Desktop/kaggle/kaggle_competetion/Google_Quest_LABEL/my-solution/input/google-quest-challenge/sample_submission.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n",
       "       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n",
       "       'url', 'category', 'host', 'question_asker_intent_understanding',\n",
       "       'question_body_critical', 'question_conversational',\n",
       "       'question_expect_short_answer', 'question_fact_seeking',\n",
       "       'question_has_commonly_accepted_answer',\n",
       "       'question_interestingness_others', 'question_interestingness_self',\n",
       "       'question_multi_intent', 'question_not_really_a_question',\n",
       "       'question_opinion_seeking', 'question_type_choice',\n",
       "       'question_type_compare', 'question_type_consequence',\n",
       "       'question_type_definition', 'question_type_entity',\n",
       "       'question_type_instructions', 'question_type_procedure',\n",
       "       'question_type_reason_explanation', 'question_type_spelling',\n",
       "       'question_well_written', 'answer_helpful',\n",
       "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
       "       'answer_satisfaction', 'answer_type_instructions',\n",
       "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
       "       'answer_well_written'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False\n",
    "                current_segment_id = 1#新增 \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def _trim_input(title, question, answer, max_sequence_length=512-1, \n",
    "                t_max_len=70-1, q_max_len=219, a_max_len=219):#???\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "\n",
    "        t = t[:t_new_len]\n",
    "        q = norm_token_length(q, q_new_len)\n",
    "        a = norm_token_length(a, a_new_len)\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "def norm_token_length(tokens, l):\n",
    "    if len(tokens) > l:\n",
    "        head = l//2\n",
    "        tail = l - head\n",
    "        return tokens[:head] + tokens[-tail:]\n",
    "    else:\n",
    "        return tokens[:l]\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, cate, max_sequence_length=512):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    #stoken = [\"[CLS]\"] + [cate] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "    stoken_1 = [\"[CLS]\"] + [cate] + title + [\"[SEP]\"] + question + [\"[SEP]\"]\n",
    "    stoken_2 = [\"[CLS]\"] + [cate]+title + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "    input_ids = _get_ids(stoken_1, tokenizer, max_sequence_length)\n",
    "    input_ids_2 = _get_ids(stoken_2, tokenizer, max_sequence_length)\n",
    "    input_segments = _get_segments(stoken_1, max_sequence_length)\n",
    "    input_segments_2 = _get_segments(stoken_2, max_sequence_length)\n",
    "    \n",
    "    #return [input_ids, input_segments]\n",
    "    return input_ids, input_segments,input_ids_2,input_segments_2\n",
    "\n",
    "def convert_row(row,pretrained_weights):\n",
    "    #c = f\"[{row['category'].lower()}]\"\n",
    "\n",
    "    if pretrained_weights == \"bert-base-uncased\":\n",
    "        c = f\"[{row['category'].lower()}]\"#type:str\n",
    "    elif pretrained_weights == \"bert-base-cased\":\n",
    "        c = f\"[{row['category']}]\"#type:str\n",
    "    elif pretrained_weights == \"xlnet-base-cased\":\n",
    "        c = f\"[{row['category']}]\"#type:str\n",
    "    t, q, a = title = row[\"question_title\"], row[\"question_body\"], row[\"answer\"]#type:str\n",
    "\n",
    "\n",
    "    t, q, a = row[\"question_title\"], row[\"question_body\"], row[\"answer\"]\n",
    "    t, q, a = _trim_input(t, q, a)\n",
    "    #ids, segments = _convert_to_bert_inputs(t, q, a, c)\n",
    "    ids, segments, ids2, segments2 = _convert_to_bert_inputs(t, q, a, c)\n",
    "    #total_input=[np.array([[ids, segments]]),np.array([[ids2, segments2]])]\n",
    "    # total_input=[]\n",
    "    # print(np.array([[ids, segments]]).shape)\n",
    "    # total_input.append(np.array([[ids, segments]]))\n",
    "    # total_input.append(np.array([[ids2, segments2]]))\n",
    "    # return total_input\n",
    "    return np.array([[ids, segments, ids2, segments2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=\"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./double/token_model_config/tokenizer/\")\n",
    "#tokenizer.add_tokens(\"./bert_based_tokenizer/added_tokens.json\")\n",
    "#tokenizer.add_tokens(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data,data2, targets,targets2):\n",
    "    \n",
    "    # mse = nn.MSELoss(reduction=\"none\")(data[:,:30].sigmoid(), targets[:,:30])\n",
    "    # bce = nn.BCEWithLogitsLoss(reduction='none')(data[:,:30], targets[:,:30])#??\n",
    "    mse = nn.MSELoss(reduction=\"none\")(data[:,:].sigmoid(), targets[:,:])\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='none')(data[:,:], targets[:,:])#??\n",
    "    \n",
    "    mse2 = nn.MSELoss(reduction=\"none\")(data2[:,:].sigmoid(), targets2[:,:])\n",
    "    bce2 = nn.BCEWithLogitsLoss(reduction='none')(data2[:,:], targets2[:,:])#??\n",
    "    #w =  targets[:,30:]\n",
    "    #loss = (mse*w).sum() + bce.sum()\n",
    "    loss = (mse).sum()+ bce.sum()+mse2.sum()+bce2.sum()\n",
    "    return loss\n",
    "\n",
    "class CustomBert(nn.Module):\n",
    "    def __init__(self, config_path=None):\n",
    "        super(CustomBert, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(config_path) \n",
    "        \n",
    "        self.config.Q_labels = 21\n",
    "        self.config.A_labels = 9\n",
    "        self.config.output_hidden_states = True\n",
    "        self.n_use_layer = 4 #原本\n",
    "        #self.n_use_layer = 2\n",
    "        self.double_bert= 1\n",
    "        self.n_labels = self.config.num_labels\n",
    "        #self.config.save_pretrained(\"bert_based_config\")\n",
    "        #self.config.save_pretrained(output_dir+\"config\")\n",
    "        #self.bert = BertModel(config)\n",
    "        self.bert=AutoModel.from_config(self.config)\n",
    "        self.bert2=AutoModel.from_config(self.config)\n",
    "        #self.bert.save_pretrained('bert_based_model')\n",
    "        #self.bert.save_pretrained(output_dir+\"model\")\n",
    "        # self.dense1 = nn.Linear(self.config.hidden_size*self.n_use_layer, self.config.hidden_size*self.n_use_layer)\n",
    "        # self.dense2 = nn.Linear(self.config.hidden_size*self.n_use_layer, self.config.hidden_size*self.n_use_layer)\n",
    "        # self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        # self.classifier = nn.Linear(self.config.hidden_size*self.n_use_layer, self.config.num_labels)\n",
    "\n",
    "        self.dense1 = nn.Linear(self.double_bert*self.config.hidden_size*self.n_use_layer, self.double_bert*self.config.hidden_size*self.n_use_layer)\n",
    "        self.dense2 = nn.Linear(self.double_bert*self.config.hidden_size*self.n_use_layer, self.double_bert*self.config.hidden_size*self.n_use_layer)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.double_bert*self.config.hidden_size*self.n_use_layer, self.config.Q_labels)\n",
    "        self.classifier2 = nn.Linear(self.double_bert*self.config.hidden_size*self.n_use_layer, self.config.A_labels)\n",
    "        #self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None\n",
    "                ,input_ids2=None, attention_mask2=None, token_type_ids2=None,position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "\n",
    "        # outputs = self.bert(input_ids,\n",
    "        #                     attention_mask=attention_mask,\n",
    "        #                     token_type_ids=token_type_ids,\n",
    "        #                     position_ids=position_ids,\n",
    "        #                     head_mask=head_mask,\n",
    "        #                     inputs_embeds=inputs_embeds)\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                            )\n",
    "        outputs2 = self.bert2(input_ids2,\n",
    "                            attention_mask=attention_mask2,\n",
    "                            token_type_ids=token_type_ids2\n",
    "                            )\n",
    "        \n",
    "        #print(outputs[2][-1].shape)\n",
    "        pooled_output = torch.cat([outputs[2][-1*(i+1)][:,0] for i in range(self.n_use_layer)], dim=1)#把倒數最後4個layer的cls output concat在一起，把4個(8,768) concat，變成(8,3072) #原本\n",
    "        pooled_output2 = torch.cat([outputs2[2][-1*(i+1)][:,0] for i in range(self.n_use_layer)], dim=1)\n",
    "        #pooled_output = torch.cat([outputs[2][-1*(i+1)][:,0] for i in range(self.n_use_layer)], dim=1)#把倒數最後2個layer的cls output concat在一起,把2個(8,768) concat，變成(8,1536)\n",
    "        #pooled_output2 = torch.cat([outputs2[2][-1*(i+1)][:,0] for i in range(self.n_use_layer)], dim=1)#同上\n",
    "        #double_pooled_output=torch.cat([pooled_output,pooled_output],dim=1)#(8,3072)\n",
    "        \n",
    "        pooled_output = self.dense1(pooled_output)\n",
    "        pooled_output = self.dense2(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        pooled_output2 = self.dense1(pooled_output2)\n",
    "        pooled_output2 = self.dense2(pooled_output2)\n",
    "        pooled_output2 = self.dropout(pooled_output2)\n",
    "        logits2 = self.classifier2(pooled_output2)\n",
    "\n",
    "        # double_pooled_output = self.dense1(double_pooled_output)\n",
    "        # double_pooled_output = self.dense2(double_pooled_output)\n",
    "        # double_pooled_output = self.dropout(double_pooled_output)\n",
    "        # logits = self.classifier(double_pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        outputs2 = (logits2,) + outputs2[2:]\n",
    "\n",
    "        return outputs,outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CustomBert(\"double/token_model_config/config/config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = 'bert-base-cased'\n",
    "#X_test = test[[\"question_title\", \"question_body\", \"answer\", \"category\"]].progress_apply(lambda x: convert_row(x, pretrained_weights), axis=1).values#shape(476)\n",
    "X_train = train[[\"question_title\", \"question_body\", \"answer\", \"category\"]].apply(lambda x: convert_row(x, pretrained_weights), axis=1).values#shape(476)\n",
    "#np.vstack(X_test).shape : (476, 2, 512)\n",
    "X_train = np.vstack(X_train).reshape((len(X_train), 2048))\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 2048)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29001, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert2): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29001, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dense1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "  (dense2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=3072, out_features=21, bias=True)\n",
       "  (classifier2): Linear(in_features=3072, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.bert.resize_token_embeddings(len(tokenizer))#??\n",
    "model.bert2.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir_target = \"../input/Optimize binning/bert-base-cased\"#\n",
    "\n",
    "cased_pred_lst = []\n",
    "for fold in range(10):\n",
    "    # if fold in [0,1,2,3,4,5,6,7]:\n",
    "    #     continue\n",
    "    #bert_path = f\"{model_dir_target}/bert-base-cased_f{fold}_best\"\n",
    "    #bert_path=f\"./DoubleBertBasedCase/bce_no_opt_binning/double-bert-based-case_f{fold}_best\"\n",
    "    bert_path=f\"./double/Bce-NoOptbinning/double-bert-based-case_f{fold}_best\"\n",
    "    model.load_state_dict(torch.load(bert_path),strict=False)\n",
    "    \n",
    "    lst = []\n",
    "    for i, (x_batch,)  in enumerate(train_loader):\n",
    "        # input_ids = x_batch[:, :512]\n",
    "        # token_ids = x_batch[:, 512:]\n",
    "        input_ids = x_batch[:, :512]\n",
    "        token_ids = x_batch[:, 512:1024]\n",
    "        input_ids2 = x_batch[:, 1024:1536]\n",
    "        token_ids2 = x_batch[:, 1536:]\n",
    "        #pred = model(input_ids.to(device), attention_mask=(input_ids > 0).to(device), token_type_ids=token_ids.to(device))\n",
    "        pred, pred2 = model(input_ids.to(device), attention_mask=(input_ids > 0).to(device), token_type_ids=token_ids.to(device),\n",
    "                     input_ids2=input_ids2.to(device),attention_mask2=(input_ids2 > 0).to(device),token_type_ids2=token_ids2.to(device))\n",
    "        total_y_pred=torch.cat((pred[0],pred2[0]),dim=1)\n",
    "        lst.append(total_y_pred.detach().cpu().squeeze().numpy())\n",
    "    train_pred = np.vstack(lst)#shape:(476, 30)\n",
    "    \n",
    "    cased_pred_lst.append(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cased_pred_lst[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.8190074 ,  0.67273927, -1.9348522 , ..., -1.7949765 ,\n",
       "         -0.5725745 ,  2.298391  ],\n",
       "        [ 3.3179762 ,  1.2740295 , -3.0002618 , ..., -4.814456  ,\n",
       "         -1.1427383 ,  2.4682055 ],\n",
       "        [ 2.6111875 , -0.26108506, -5.2788396 , ..., -1.4412452 ,\n",
       "          0.93280005,  2.467551  ],\n",
       "        ...,\n",
       "        [ 2.2947912 ,  0.3786489 , -5.8437157 , ..., -1.959887  ,\n",
       "         -1.4604295 ,  1.0067284 ],\n",
       "        [ 2.7061436 ,  0.4583198 ,  0.47497526, ..., -2.1556973 ,\n",
       "          2.0433908 ,  2.8822246 ],\n",
       "        [ 3.6477284 ,  2.156683  , -1.9931842 , ..., -2.8336232 ,\n",
       "          3.1300957 ,  2.2724154 ]], dtype=float32),\n",
       " array([[ 2.710054  ,  1.3484956 , -2.435326  , ..., -2.245387  ,\n",
       "          2.367202  ,  2.9308908 ],\n",
       "        [ 3.2443113 ,  1.9807558 , -4.2785535 , ..., -4.4238772 ,\n",
       "         -0.70564103,  2.6289072 ],\n",
       "        [ 2.4669995 , -0.13381238, -6.0889306 , ..., -1.9061047 ,\n",
       "          2.2147102 ,  2.6575506 ],\n",
       "        ...,\n",
       "        [ 2.230933  ,  0.18699805, -6.1866446 , ..., -1.6461258 ,\n",
       "         -1.3252726 ,  1.212281  ],\n",
       "        [ 2.7108169 ,  0.39361402,  0.28499967, ..., -1.7835851 ,\n",
       "          1.1207765 ,  2.6155627 ],\n",
       "        [ 3.6700873 ,  1.8371167 , -2.2636728 , ..., -3.998669  ,\n",
       "          5.2130256 ,  2.4134    ]], dtype=float32),\n",
       " array([[ 2.7619076 ,  1.0427129 , -1.6852927 , ..., -1.5533626 ,\n",
       "          0.36148706,  2.9192574 ],\n",
       "        [ 3.5520186 ,  1.7296226 , -3.9294844 , ..., -4.514631  ,\n",
       "          0.33651555,  2.53669   ],\n",
       "        [ 2.7536314 , -0.01586969, -6.07761   , ..., -2.5424635 ,\n",
       "          2.5421176 ,  2.6182477 ],\n",
       "        ...,\n",
       "        [ 2.262899  ,  0.36577588, -5.774676  , ..., -2.116268  ,\n",
       "         -0.57440156,  1.8171613 ],\n",
       "        [ 2.8862624 ,  0.3779696 ,  0.6586797 , ..., -1.7523661 ,\n",
       "          1.7637488 ,  2.5233305 ],\n",
       "        [ 4.4061375 ,  2.3349838 , -1.8110046 , ..., -3.673415  ,\n",
       "          4.5748267 ,  2.3240929 ]], dtype=float32),\n",
       " array([[ 3.5338886 ,  1.3801342 , -2.0897193 , ..., -1.9678378 ,\n",
       "         -0.28045633,  2.6398861 ],\n",
       "        [ 2.9734747 ,  1.9592443 , -3.8496256 , ..., -3.5290709 ,\n",
       "          0.7714671 ,  2.4447122 ],\n",
       "        [ 2.5744524 ,  0.27107126, -6.496645  , ..., -3.0912986 ,\n",
       "          2.4819071 ,  2.565661  ],\n",
       "        ...,\n",
       "        [ 2.0835018 ,  0.22888383, -5.729082  , ..., -2.3862176 ,\n",
       "          0.2562506 ,  1.924124  ],\n",
       "        [ 2.835029  ,  0.41991404,  0.24005093, ..., -2.5671613 ,\n",
       "          1.4540838 ,  2.4988198 ],\n",
       "        [ 4.0042086 ,  2.257034  , -1.8849947 , ..., -2.6374195 ,\n",
       "          4.473446  ,  2.4972196 ]], dtype=float32),\n",
       " array([[ 2.8805876 ,  1.0537701 , -2.6012118 , ..., -1.6511394 ,\n",
       "          0.50296986,  2.6619492 ],\n",
       "        [ 3.4015405 ,  2.0224123 , -4.8483877 , ..., -3.2690651 ,\n",
       "         -0.46678805,  2.4351444 ],\n",
       "        [ 2.56365   , -0.07771987, -6.2341747 , ..., -1.9121155 ,\n",
       "          2.228872  ,  2.637716  ],\n",
       "        ...,\n",
       "        [ 2.426239  ,  0.45047507, -5.861055  , ..., -2.4543474 ,\n",
       "         -3.1285453 ,  1.6479167 ],\n",
       "        [ 2.493726  ,  0.15030052, -0.06005504, ..., -1.7114054 ,\n",
       "          1.6295347 ,  2.5002656 ],\n",
       "        [ 3.6091511 ,  2.3605218 , -2.0924058 , ..., -3.4439886 ,\n",
       "          4.451272  ,  2.743917  ]], dtype=float32),\n",
       " array([[ 3.0815015 ,  0.8108574 , -2.0151322 , ..., -2.1531498 ,\n",
       "          0.32825792,  2.597377  ],\n",
       "        [ 2.7505844 ,  1.4732938 , -4.492465  , ..., -4.46273   ,\n",
       "          0.40012598,  2.1841462 ],\n",
       "        [ 2.3320105 ,  0.24730363, -6.0077176 , ..., -3.12644   ,\n",
       "          2.355017  ,  2.4345827 ],\n",
       "        ...,\n",
       "        [ 2.19811   ,  0.2869531 , -6.003982  , ..., -2.3051243 ,\n",
       "         -2.96563   ,  1.161373  ],\n",
       "        [ 2.698278  ,  0.65723616,  0.7439947 , ..., -2.2630198 ,\n",
       "          2.1552763 ,  2.6740546 ],\n",
       "        [ 3.5454893 ,  2.4126275 , -2.314893  , ..., -2.9597547 ,\n",
       "          2.7645278 ,  2.7259266 ]], dtype=float32),\n",
       " array([[ 2.8022592e+00,  7.4027205e-01, -2.4334245e+00, ...,\n",
       "         -1.8467330e+00, -5.0571080e-02,  2.9416881e+00],\n",
       "        [ 2.7990720e+00,  1.6725199e+00, -4.2002072e+00, ...,\n",
       "         -3.7874475e+00,  5.6726640e-01,  2.5873518e+00],\n",
       "        [ 2.6428945e+00, -2.3882668e-01, -6.4565582e+00, ...,\n",
       "         -2.3816299e+00,  3.0381658e+00,  2.7165029e+00],\n",
       "        ...,\n",
       "        [ 2.2616043e+00,  3.1047666e-01, -6.1573968e+00, ...,\n",
       "         -2.4176238e+00, -2.5540040e+00,  1.1333674e+00],\n",
       "        [ 2.9383206e+00,  4.7471398e-01, -5.0925347e-03, ...,\n",
       "         -1.5992887e+00,  1.3146187e+00,  2.7462814e+00],\n",
       "        [ 3.3470850e+00,  1.8994317e+00, -1.7865984e+00, ...,\n",
       "         -4.2804694e+00,  4.7338781e+00,  2.5726860e+00]], dtype=float32),\n",
       " array([[ 2.8947504 ,  1.1198765 , -2.4084225 , ..., -1.8173691 ,\n",
       "         -2.091456  ,  2.3382483 ],\n",
       "        [ 3.3864598 ,  2.1778142 , -4.266247  , ..., -4.451142  ,\n",
       "          0.10767537,  2.6090047 ],\n",
       "        [ 2.6373158 , -0.09588078, -5.6781883 , ..., -2.0286038 ,\n",
       "          2.3177164 ,  2.5312426 ],\n",
       "        ...,\n",
       "        [ 2.277895  ,  0.14796121, -5.998359  , ..., -2.376719  ,\n",
       "         -1.1229208 ,  1.6926855 ],\n",
       "        [ 2.9176116 ,  1.1654855 ,  0.17767213, ..., -2.069021  ,\n",
       "          2.84277   ,  2.703639  ],\n",
       "        [ 3.5733867 ,  2.126689  , -2.6070964 , ..., -3.0295908 ,\n",
       "          4.1303196 ,  2.4351962 ]], dtype=float32),\n",
       " array([[ 2.7172079e+00,  8.3686322e-01, -2.3051300e+00, ...,\n",
       "         -1.8849375e+00,  6.7123592e-01,  2.4499683e+00],\n",
       "        [ 3.2458680e+00,  1.7166004e+00, -3.9843924e+00, ...,\n",
       "         -4.0698171e+00,  1.6148130e+00,  2.4902544e+00],\n",
       "        [ 2.4886141e+00, -4.6159867e-03, -6.0147324e+00, ...,\n",
       "         -2.4812870e+00,  2.2153406e+00,  2.5218234e+00],\n",
       "        ...,\n",
       "        [ 2.3670042e+00,  3.8331261e-01, -5.7669950e+00, ...,\n",
       "         -1.8738142e+00, -3.3193552e+00,  1.4315429e+00],\n",
       "        [ 3.0258858e+00,  5.7365316e-01,  2.3711491e-01, ...,\n",
       "         -1.1850749e+00,  1.7533308e+00,  2.6871278e+00],\n",
       "        [ 3.7586665e+00,  2.2192702e+00, -1.6219833e+00, ...,\n",
       "         -4.4434924e+00,  5.0410819e+00,  2.3580489e+00]], dtype=float32),\n",
       " array([[ 3.0172594e+00,  7.8845584e-01, -1.5684286e+00, ...,\n",
       "         -1.6353048e+00, -1.6372721e+00,  2.6748054e+00],\n",
       "        [ 3.3410645e+00,  1.6258107e+00, -4.8682108e+00, ...,\n",
       "         -4.2260914e+00,  5.5619958e-04,  2.7277398e+00],\n",
       "        [ 2.5199804e+00, -3.9828068e-01, -6.3702102e+00, ...,\n",
       "         -2.8339608e+00,  2.1942656e+00,  3.0085549e+00],\n",
       "        ...,\n",
       "        [ 1.9968377e+00, -8.8220760e-03, -5.7908473e+00, ...,\n",
       "         -2.1219723e+00,  1.8486209e+00,  1.9267535e+00],\n",
       "        [ 2.5056818e+00,  3.8874018e-01, -3.2597616e-01, ...,\n",
       "         -2.0090420e+00,  1.3990920e+00,  2.7400663e+00],\n",
       "        [ 2.8250811e+00,  1.1872401e+00, -2.0953290e+00, ...,\n",
       "         -2.4629755e+00,  2.8215792e+00,  2.8565423e+00]], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cased_pred_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[1,2,3],[4,5,6]]#(2,3)\n",
    "#b=[[7,8,9],[10,11,12]]\n",
    "a=np.array(a)\n",
    "#b=np.array(b)\n",
    "c=[]\n",
    "c.append(a)\n",
    "#c.append(b)\n",
    "np.array(c).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.9218423 ,  0.9794178 , -2.1476939 , ..., -1.8550199 ,\n",
       "        -0.04011768,  2.6452463 ],\n",
       "       [ 3.2012367 ,  1.7632103 , -4.1717834 , ..., -4.1548333 ,\n",
       "         0.14832522,  2.5112154 ],\n",
       "       [ 2.5590737 , -0.07077162, -6.0703607 , ..., -2.374515  ,\n",
       "         2.252091  ,  2.6159432 ],\n",
       "       ...,\n",
       "       [ 2.2399817 ,  0.27306634, -5.9112754 , ..., -2.1658099 ,\n",
       "        -1.4345688 ,  1.4953934 ],\n",
       "       [ 2.7717757 ,  0.5059947 ,  0.24263635, ..., -1.9095663 ,\n",
       "         1.7476622 ,  2.6571372 ],\n",
       "       [ 3.638702  ,  2.0791597 , -2.047116  , ..., -3.3763397 ,\n",
       "         4.1334047 ,  2.5199447 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cased_pred_lst).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94891566, 0.72699267, 0.10454692, ..., 0.13528457, 0.48997197,\n",
       "        0.9337174 ],\n",
       "       [0.96088076, 0.8536113 , 0.01519042, ..., 0.01544608, 0.5370135 ,\n",
       "        0.9249244 ],\n",
       "       [0.92818075, 0.48231447, 0.00230501, ..., 0.08513682, 0.90483075,\n",
       "        0.93188065],\n",
       "       ...,\n",
       "       [0.90378284, 0.5678455 , 0.00270141, ..., 0.10286307, 0.1923878 ,\n",
       "        0.81688637],\n",
       "       [0.9411315 , 0.6238671 , 0.5603633 , ..., 0.12902959, 0.85165775,\n",
       "        0.93444955],\n",
       "       [0.9743868 , 0.888861  , 0.11434411, ..., 0.03304315, 0.9842247 ,\n",
       "        0.92552817]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "cased_pred = np.array(cased_pred_lst).mean(0)\n",
    "cased_pred = sigmoid(cased_pred)\n",
    "cased_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names3= list(sub.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cased_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = train[class_names3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#26th PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_asker_intent_understanding': 18,\n",
       " 'question_body_critical': 18,\n",
       " 'question_conversational': 6,\n",
       " 'question_expect_short_answer': 6,\n",
       " 'question_fact_seeking': 6,\n",
       " 'question_has_commonly_accepted_answer': 6,\n",
       " 'question_interestingness_others': 18,\n",
       " 'question_interestingness_self': 18,\n",
       " 'question_multi_intent': 6,\n",
       " 'question_not_really_a_question': 6,\n",
       " 'question_opinion_seeking': 6,\n",
       " 'question_type_choice': 6,\n",
       " 'question_type_compare': 6,\n",
       " 'question_type_consequence': 6,\n",
       " 'question_type_definition': 6,\n",
       " 'question_type_entity': 6,\n",
       " 'question_type_instructions': 6,\n",
       " 'question_type_procedure': 6,\n",
       " 'question_type_reason_explanation': 6,\n",
       " 'question_type_spelling': 3,\n",
       " 'question_well_written': 18,\n",
       " 'answer_helpful': 18,\n",
       " 'answer_level_of_information': 18,\n",
       " 'answer_plausible': 18,\n",
       " 'answer_relevance': 18,\n",
       " 'answer_satisfaction': 30,\n",
       " 'answer_type_instructions': 6,\n",
       " 'answer_type_procedure': 6,\n",
       " 'answer_type_reason_explanation': 6,\n",
       " 'answer_well_written': 18}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict = {}\n",
    "for c in sub.columns[1:]:\n",
    "  unique_values = train[c].unique()\n",
    "  unique_values = list(set(list(unique_values)+[0,1]))\n",
    "  lst = []\n",
    "  for common_num in range(90):#90? 0\n",
    "    num = 90 - common_num#90\n",
    "    bunbo = [round((1/num)*n, 8) for n in range(num+1)]\n",
    "    #print(len(bunbo))\n",
    "    kyoutu = [round(v, 8) for v in unique_values if round(v, 8) in bunbo]\n",
    "    #print(kyoutu)\n",
    "    if len(kyoutu) == len(unique_values):\n",
    "      lst.append(num)\n",
    "  norm_dict[c] = min(lst)\n",
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict[\"question_asker_intent_understanding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.000000\n",
       "1       1.000000\n",
       "2       0.888889\n",
       "3       0.888889\n",
       "4       1.000000\n",
       "          ...   \n",
       "6074    1.000000\n",
       "6075    1.000000\n",
       "6076    0.888889\n",
       "6077    1.000000\n",
       "6078    1.000000\n",
       "Name: question_asker_intent_understanding, Length: 6079, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"question_asker_intent_understanding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.88888889, ..., 0.88888889, 1.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3[:,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4TH Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1th PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy2=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_columns:\n",
    "    test_copy2[target]=cased_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessed=postprocess_prediction(train,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.602632</td>\n",
       "      <td>0.082691</td>\n",
       "      <td>0.425644</td>\n",
       "      <td>0.664275</td>\n",
       "      <td>0.655471</td>\n",
       "      <td>0.707268</td>\n",
       "      <td>0.669800</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947240</td>\n",
       "      <td>0.972848</td>\n",
       "      <td>0.715149</td>\n",
       "      <td>0.991980</td>\n",
       "      <td>0.992476</td>\n",
       "      <td>0.928810</td>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.950756</td>\n",
       "      <td>0.953038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.884563</td>\n",
       "      <td>0.476672</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.742485</td>\n",
       "      <td>0.994332</td>\n",
       "      <td>0.540612</td>\n",
       "      <td>0.412230</td>\n",
       "      <td>0.099950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.983204</td>\n",
       "      <td>0.682555</td>\n",
       "      <td>0.993719</td>\n",
       "      <td>0.994706</td>\n",
       "      <td>0.920328</td>\n",
       "      <td>0.979071</td>\n",
       "      <td>0.041499</td>\n",
       "      <td>0.024477</td>\n",
       "      <td>0.863808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.950187</td>\n",
       "      <td>0.658466</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.696439</td>\n",
       "      <td>0.949114</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>0.607235</td>\n",
       "      <td>0.509221</td>\n",
       "      <td>0.411673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935586</td>\n",
       "      <td>0.985808</td>\n",
       "      <td>0.730065</td>\n",
       "      <td>0.995348</td>\n",
       "      <td>0.992168</td>\n",
       "      <td>0.945892</td>\n",
       "      <td>0.093498</td>\n",
       "      <td>0.042117</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.948664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.938729</td>\n",
       "      <td>0.525467</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.762707</td>\n",
       "      <td>0.837885</td>\n",
       "      <td>0.890279</td>\n",
       "      <td>0.553451</td>\n",
       "      <td>0.437848</td>\n",
       "      <td>0.049915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779918</td>\n",
       "      <td>0.947412</td>\n",
       "      <td>0.658828</td>\n",
       "      <td>0.973937</td>\n",
       "      <td>0.982233</td>\n",
       "      <td>0.886768</td>\n",
       "      <td>0.884983</td>\n",
       "      <td>0.145141</td>\n",
       "      <td>0.473806</td>\n",
       "      <td>0.905280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.945720</td>\n",
       "      <td>0.569130</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.893930</td>\n",
       "      <td>0.744055</td>\n",
       "      <td>0.986870</td>\n",
       "      <td>0.572701</td>\n",
       "      <td>0.508597</td>\n",
       "      <td>0.111417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502510</td>\n",
       "      <td>0.941061</td>\n",
       "      <td>0.680172</td>\n",
       "      <td>0.984167</td>\n",
       "      <td>0.972880</td>\n",
       "      <td>0.852857</td>\n",
       "      <td>0.290666</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.774137</td>\n",
       "      <td>0.900799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.949099                0.602632   \n",
       "1     46                             0.884563                0.476672   \n",
       "2     70                             0.950187                0.658466   \n",
       "3    132                             0.938729                0.525467   \n",
       "4    200                             0.945720                0.569130   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.082691                      0.425644   \n",
       "1                 0.000969                      0.846933   \n",
       "2                 0.003706                      0.696439   \n",
       "3                 0.002455                      0.762707   \n",
       "4                 0.014231                      0.893930   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.664275                               0.655471   \n",
       "1               0.742485                               0.994332   \n",
       "2               0.949114                               0.970600   \n",
       "3               0.837885                               0.890279   \n",
       "4               0.744055                               0.986870   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.707268                       0.669800   \n",
       "1                         0.540612                       0.412230   \n",
       "2                         0.607235                       0.509221   \n",
       "3                         0.553451                       0.437848   \n",
       "4                         0.572701                       0.508597   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.798551  ...               0.947240        0.972848   \n",
       "1               0.099950  ...               0.515464        0.983204   \n",
       "2               0.411673  ...               0.935586        0.985808   \n",
       "3               0.049915  ...               0.779918        0.947412   \n",
       "4               0.111417  ...               0.502510        0.941061   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.715149          0.991980          0.992476   \n",
       "1                     0.682555          0.993719          0.994706   \n",
       "2                     0.730065          0.995348          0.992168   \n",
       "3                     0.658828          0.973937          0.982233   \n",
       "4                     0.680172          0.984167          0.972880   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.928810                  0.097158               0.026617   \n",
       "1             0.920328                  0.979071               0.041499   \n",
       "2             0.945892                  0.093498               0.042117   \n",
       "3             0.886768                  0.884983               0.145141   \n",
       "4             0.852857                  0.290666               0.074808   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.950756             0.953038  \n",
       "1                        0.024477             0.863808  \n",
       "2                        0.930348             0.948664  \n",
       "3                        0.473806             0.905280  \n",
       "4                        0.774137             0.900799  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.columns[1:]] = cased_pred\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "490296f01f129d1791d2b0949f01d8bcbb5336528260279af7e468eefab7c316"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
